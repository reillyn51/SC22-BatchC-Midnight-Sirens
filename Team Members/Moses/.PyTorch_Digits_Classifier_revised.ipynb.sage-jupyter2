{"backend_state":"init","connection_file":"/projects/7054eb54-2282-47cc-9fd4-f73d8c0d026c/.local/share/jupyter/runtime/kernel-f9257482-bd23-491d-af9d-809cd3971a3c.json","kernel":"cv_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Digits_Classifier.ipynb","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1658771699980,"exec_count":2,"id":"148e01","input":"for i in range(10):\n    rand_idx = random.randint(0, len(trainset)-1) # Pick an index for a random image\n    plt.subplot(2, 5, i+1) # 2 * 5 images in the grid, display 10 total\n    plt.axis('off')\n    # [1, 28, 28] -> [28, 28]\n    plt.imshow(trainset[rand_idx][0].squeeze(0))","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"AwWg6nMm4gdg","outputId":"bfd04d34-6ca8-43bc-8830-e3978ec1316e"},"output":{"0":{"ename":"NameError","evalue":"name 'random' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     rand_idx \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trainset)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Pick an index for a random image\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 2 * 5 images in the grid, display 10 total\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"]}},"pos":9,"start":1658771699964,"state":"done","type":"cell"}
{"cell_type":"code","end":1658771871520,"exec_count":4,"id":"27ba4e","input":"trainset = torchvision.datasets.MNIST('train_set', download=True, train=True, transform=transform) # downloads to train_set\nvalset = torchvision.datasets.MNIST('test_set', download=True, train=False, transform=transform) # downloads to test_set\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # batch is the number of images to consider at a time\nvalloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)","kernel":"cv_env","metadata":{"id":"Q_fPpFanuNAa"},"output":{"0":{"ename":"NameError","evalue":"name 'transform' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_set\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39m\u001b[43mtransform\u001b[49m) \u001b[38;5;66;03m# downloads to train_set\u001b[39;00m\n\u001b[1;32m      2\u001b[0m valset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_set\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform) \u001b[38;5;66;03m# downloads to test_set\u001b[39;00m\n\u001b[1;32m      3\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# batch is the number of images to consider at a time\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'transform' is not defined"]}},"pos":7,"start":1658771871505,"state":"done","type":"cell"}
{"cell_type":"code","end":1658771962436,"exec_count":5,"id":"b135b4","input":"import random\nimport torch\nimport torchvision\nimport matplotlib.pyplot as plt","kernel":"cv_env","metadata":{"id":"LtoR_ZZwuDQj"},"pos":3,"start":1658771962414,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"044e1a","input":"# get and show a sample image\nimage = valset[0][0] # shape: [(1) batch_size, 28, 28]\nplt.subplot()\nplt.axis('off')\n# [1, 28, 28] -> [28, 28] - makes the image readable\nplt.imshow(image.squeeze(0))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"9CrgNFgI3xYt","outputId":"3b10d270-b394-4a36-f944-2b57006315cb"},"output":{"0":{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f84769e7690>"},"execution_count":0,"metadata":{},"output_type":"execute_result"},"1":{"data":{"image/png":"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","text/plain":"<Figure size 432x288 with 1 Axes>"},"metadata":{"needs_background":"light"},"output_type":"display_data"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1f5691","input":"","pos":21,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6b81ca","input":"correct = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for images, labels in valloader:\n        #EXERCISE: Transfer the inputs and labels to the GPU\n        #EXERCISE: calculate outputs by running images through the network\n        \n        images = images.view(images.shape[0], -1)\n        # calculate outputs by running images through the network\n        outputs = model(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ppm0FK4xyJQR","outputId":"687c617f-975b-4636-f2e6-f3f321837f1b"},"output":{"0":{"name":"stdout","output_type":"stream","text":"Number Of Images Tested = 10000\n\nModel Accuracy = 0.9664\n"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"717eac","input":"# [(1) batch_size, 28, 28]-> [1(batch size), 784]\nimage = image.view(1, 784)\nwith torch.no_grad():\n    # cast input to device\n    image = image.to(device)\n\n    log_preds = model(image) # runs the model\n\n# post processes the image into probabilities of it being each digit\n#   Math: probabilites were natural logged, so torch.exp() performs e^(log_preds)\npreds = torch.exp(log_preds)\nprobab = list(preds.cpu().numpy()[0])\n\n# the index, this time, is the same as the output, so we can just grab and print it\npred_label = probab.index(max(probab)) # get index of highest num (highest probability)\nprint(f\"Prediction: {pred_label}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hxLIQGI6HHa","outputId":"53825ba4-27c7-424b-d6c9-63a7eccbdcf2"},"output":{"0":{"name":"stdout","output_type":"stream","text":"Prediction: 7\n"}},"pos":31,"type":"cell"}
{"cell_type":"code","exec_count":18,"id":"2adccf","input":"model = torch.nn.Sequential(\n    # [(64) batch_size, (768) width x height] -> [(64) batch_size, (128) hidden_size #1]\n    torch.nn.Linear(input_size, hidden_sizes[0]),\n    torch.nn.ReLU(), # activation function\n    # [(64) batch_size, (128) hidden_size #1] -> [(64) batch_size, (32) hidden_size #2]\n    torch.nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n    torch.nn.ReLU(), # activation function\n    # [(64) batch_size, (32) hidden_size #2] -> [(64) batch_size, (10) num_class]\n\n    #EXERCISE: Define the third layer as taking in the output size of the second layer and outputting the number of classes. \n    torch.nn.Linear(hidden_sizes[1], num_class),\n    # output activation function - the hidden layer functions don't work for optimization\n    torch.nn.ReLU(),\n    # LogSoftmax because it is better at gradient optimization\n    torch.nn.LogSoftmax(dim=-1) # apply LogSoftmax to the last layer (num_class)\n)","metadata":{"id":"iWS_Hzt-uhNP"},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"bad69c","input":"model.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqtJo49FxLH-","outputId":"3b8bc56d-ab7f-4a57-da50-4eb6d2f4651e"},"output":{"0":{"data":{"text/plain":"Sequential(\n  (0): Linear(in_features=784, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=32, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=32, out_features=10, bias=True)\n  (5): ReLU()\n  (6): LogSoftmax(dim=-1)\n)"},"execution_count":19,"metadata":{},"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":4,"id":"56ee2c","input":"# \ntransform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n  torchvision.transforms.Normalize((0.5,), (0.5,)),\n])","metadata":{"id":"LSyJL6QIuJM4"},"pos":5,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"08a2ae","input":"# Model training for 8 times\ncriterion = torch.nn.NLLLoss() # Loss object to find back propagation\noptimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9) # Optimizer\nepochs = 8\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in trainloader:\n        #EXERCISE: transfer images and labels to GPU. \n        inputs = inputs.to(divice)\n        labels = labels.to(divice)\n        # Formats the image to be a usable 1d array.\n        #   [(64) batch size, 1, 28, 28] -> [(64) batch size, 768]\n        images = images.view(images.shape[0], -1)\n\n        # Resets the optimizer for each training step\n        optimizer.zero_grad()\n\n        # Finds error then runs back propagation\n        output = model(images)\n        #EXERCISE: calculate the loss by passing the outputs and the labels into the lsos function. \n        loss = criterion(output,labels)\n\n        loss.backward()\n\n        # Updates model weights\n        optimizer.step()\n\n        # Keeps track of error to allow visualization of progress\n        running_loss += loss.item()\n    else:\n        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))","kernel":"cv_env","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-B0laEkwlst","outputId":"b6c2fd03-4761-42fe-ad5f-7087096b3a88"},"output":{"0":{"ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model training for 8 times\u001b[39;00m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mNLLLoss() \u001b[38;5;66;03m# Loss object to find back propagation\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m) \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}},"pos":24,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":8,"id":"b62a50","input":"input_size = 28 * 28 # width times height of the image (number of pixels)\nhidden_sizes = [128, 32] # this is the sizes of the hidden layers. The sizes are relativly arbitrary\nnum_class = 10 # one label for each digit (0-9)","metadata":{"id":"Bg20RWXHuW2W"},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"1ad9a1","input":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"ATekqXGWxKBs"},"pos":14,"type":"cell"}
{"cell_type":"markdown","id":"04411c","input":"So torch.nn.Sequential pretty much compiles a group of layers into one network, and runs them sequentially for predictions. All of the layers that we are using for this are fully connected layers. The input size is 28 by 28 because we are flattening out the 28 by 28 image into 28*28=784 numbers. We are adding a activation function ReLU after that. ReLU(x) = max(x,0) so relu turns negative values to zero and positive values stay the same. \n\nAfter the first layer, we take the number of outputs of the first layer as the number of inputs into the second layer. And an arbitrary number of outputs for the second layer that we decide. \n\nNotice how we have an activation function after each layer. \n\nThen the third layer you code yourself, view the instructions below. \n\nFinally, we have an output activation function. The 10 numbers that are outputted from our previous layer can be any numbers from negative infinity to infinity. We want every class output to be from zero to one, like a probability. The softmax activation function turns the output into a probability for each class. \n\n","metadata":{"id":"QtFMastrDkvf"},"pos":16,"type":"cell"}
{"cell_type":"markdown","id":"08ab00","input":"NLLLoss docs - https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss\nOptim docs - https://pytorch.org/docs/stable/optim.html","metadata":{"id":"wpYVqK89Dkvj"},"pos":23,"type":"cell"}
{"cell_type":"markdown","id":"158584","input":"Build a neural network in pytorch with two hidden layers","metadata":{"id":"hRpV2GTwvBYZ"},"pos":15,"type":"cell"}
{"cell_type":"markdown","id":"23cae1","input":"Run Prediction","metadata":{"id":"q2ZVmfom6XsO"},"pos":30,"type":"cell"}
{"cell_type":"markdown","id":"2646b7","input":"Got your own image to run prediction on?","metadata":{"id":"ffF-vY365-Dn"},"pos":28,"type":"cell"}
{"cell_type":"markdown","id":"34330a","input":"# Applying the transforms\n\nWe are using the datasets.MNIST function from torchvision to import the dataset. \n\nParameters to the function:\n\nThe root parameter sets the directory that we import the data to (and create it if it doesn't exist.)\n\nThe train parameter determines if we are importing training or testing fashion MNIST dataset. \n\nThe transform parameter determines the transforms we apply during preprocessing, which were defined above.\n\ndownload=True gives the function permission to download the data into the directory if it doesn't exist there. ","metadata":{"id":"MMEm9idPuYWq"},"pos":6,"type":"cell"}
{"cell_type":"markdown","id":"3f6c4b","input":"## Hyperparameters\n\ndatasets docs - https://pytorch.org/vision/stable/datasets.html\n\nThe input size is the pixel size of the images, each of which is 28 by 28.\n\nThe number of classifcation (num_classes) is 10 because there are 10 possible classifications the model can make, such as 0, 1, 2 ... 9. \n\nThe hidden_sizes is the number of neurons in the hidden layer of the neural network. The input size and output sizes are always fixed (input size is related to number of pixels and output size is the number of classes) but the optimal hidden layer sizes can be determined only through experimentation. \n\n","metadata":{"id":"_xDFVY4muq_1"},"pos":11,"type":"cell"}
{"cell_type":"markdown","id":"5b45b4","input":"<h1>Creating the Model</h1>","metadata":{"id":"pc8-Fbf4Dkvb"},"pos":10,"type":"cell"}
{"cell_type":"markdown","id":"5d892f","input":"GPU boost training time. Why? Because it lets us do many operations at the same time in a parallelized sort of way. \n\nCUDA is the API that we will use for GPU training. If CUDA is available we want to use it, and otherwise use the CPU. Google colab comes with a built in GPU for use so make sure to activate it by going to Runtime->Change runtime type->GPU\n\n","metadata":{"id":"E9WnnZwZxGWq"},"pos":13,"type":"cell"}
{"cell_type":"markdown","id":"6a6605","input":"## Training our Model\n\nThe training process goes somewhat like this\ngo through the dataset [epoch] times<br>\n&ensp;  go through each image in the dataset<br>\n&ensp;&ensp; transfer inputs and labels to GPU<br>\n&ensp;&ensp; get prediction for input<br>\n&ensp;&ensp; check if prediction matches label, get loss<br>\n&ensp;&ensp; see which direction you have to change the weights<br>\n&ensp;&ensp; actually change weights using optimizer and learning rate\n&ensp;&ensp; Set the directions back to zero (optim.zero_grad())<br>\n&ensp;&ensp; add loss to total loss until reset<br>\n&ensp;&ensp; after some iterations, print out loss and reset\n\nFew more things to note. NLLLoss is the default loss function for softmax (probabilities that are far away from the true probabilities are penalized). \n\n","metadata":{"id":"1HqqlulNwyqb"},"pos":22,"type":"cell"}
{"cell_type":"markdown","id":"71c66e","input":"# Classifying handwritten digits (0-9) with neural networks ","metadata":{"id":"LGtV5-Jc6zJz"},"pos":0,"type":"cell"}
{"cell_type":"markdown","id":"76a063","input":"We have to transfer the model to the GPU device. ","metadata":{"id":"wqegJQ24Dkvg"},"pos":18,"type":"cell"}
{"cell_type":"markdown","id":"848a34","input":"Random is the default python library for generating random numbers.\n\nPyTorch, or torch, is the python deep learning library we use for our neural networks. \n\nTorchvision is for computer vision specific functions such as transforming images and image datasets. \n\nMatplotlib is used for graphing figures with data, whether it be scatterplots, heatmaps, lineplots, etc...\n\n","metadata":{"id":"kWiyySymuN07"},"pos":2,"type":"cell"}
{"cell_type":"markdown","id":"942b53","input":"<h1>Getting Set Up</h1>\n\n","metadata":{"id":"KU8e-P7CDkvN"},"pos":1,"type":"cell"}
{"cell_type":"markdown","id":"ab306e","input":"<h1>Training the model</h1>\n\n","metadata":{"id":"N1UvEyWzDkvi"},"pos":20,"type":"cell"}
{"cell_type":"markdown","id":"cdd6fc","input":"## Visualization !!!\n\nFor 10 iterations, select a random index from zero to the length of the training dataset. Display each image in a 2 by 5 subplot as the 1st, 2nd, 3rd etc... image in the plot.\n\n","metadata":{"id":"0T4Y-JVy4WUy"},"pos":8,"type":"cell"}
{"cell_type":"markdown","id":"e38b16","input":"<h1>Preprocessing our Images</h1>\n\nThe transform that we will perform on our dataset is first converting all images to tensors. Tensors are the built in array datatype in pytorch, like numpy arrays. If interested, learn about why they are useful in keeping track of gradients here:\n[https://pytorch.org/tutorials/beginner/blitz/autograd\\_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) .  \n\nConverting to a tensor also converts an image with pixel values from 0 to 255 to a matrix with numbers from 0 to 1. \n\nIn addition, we are normalizing the data to a range between -1 and 1. If the range before is [0,1], subtracting 0.5 will give us [-0.5,0.5] and dividing by 0.5 will make the range wider to [-1,1]. torch.Normalize subtracts the first parameter from all the values in the image and divides by the second parameter. \n\n","metadata":{"id":"m2tj_3PsuRkh"},"pos":4,"type":"cell"}
{"cell_type":"markdown","id":"e9196f","input":"## Evaluation Loop\n&ensp; Iterate through every batch in the dataset<br>\n&ensp;&ensp; Get the prediction of every image in the batch<br>\n&ensp;&ensp; Add the number of images to the total<br>\n&ensp;&ensp; Add the number of correctly classified images to a counter<br>\n&ensp;&ensp; Get the accuracy through correct/total.<br>\n","metadata":{"id":"S_58fHPtDkvk"},"pos":25,"type":"cell"}
{"cell_type":"markdown","id":"f37a8d","input":"<h1>Using the Model!</h1>","metadata":{"id":"FU9DtjN9Dkvm"},"pos":27,"type":"cell"}
{"id":0,"time":1658847434281,"type":"user"}
{"last_load":1658770972140,"type":"file"}