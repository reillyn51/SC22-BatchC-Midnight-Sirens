{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "LGtV5-Jc6zJz"
   },
   "source": [
    "# Classifying handwritten digits (0-9) with neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KU8e-P7CDkvN"
   },
   "source": [
    "<h1>Getting Set Up</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kWiyySymuN07"
   },
   "source": [
    "Random is the default python library for generating random numbers.\n",
    "\n",
    "PyTorch, or torch, is the python deep learning library we use for our neural networks. \n",
    "\n",
    "Torchvision is for computer vision specific functions such as transforming images and image datasets. \n",
    "\n",
    "Matplotlib is used for graphing figures with data, whether it be scatterplots, heatmaps, lineplots, etc...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "LtoR_ZZwuDQj"
   },
   "outputs": [
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "m2tj_3PsuRkh"
   },
   "source": [
    "<h1>Preprocessing our Images</h1>\n",
    "\n",
    "The transform that we will perform on our dataset is first converting all images to tensors. Tensors are the built in array datatype in pytorch, like numpy arrays. If interested, learn about why they are useful in keeping track of gradients here:\n",
    "[https://pytorch.org/tutorials/beginner/blitz/autograd\\_tutorial.html](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) .  \n",
    "\n",
    "Converting to a tensor also converts an image with pixel values from 0 to 255 to a matrix with numbers from 0 to 1. \n",
    "\n",
    "In addition, we are normalizing the data to a range between -1 and 1. If the range before is [0,1], subtracting 0.5 will give us [-0.5,0.5] and dividing by 0.5 will make the range wider to [-1,1]. torch.Normalize subtracts the first parameter from all the values in the image and divides by the second parameter. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "LSyJL6QIuJM4"
   },
   "outputs": [
   ],
   "source": [
    "# \n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "  torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MMEm9idPuYWq"
   },
   "source": [
    "# Applying the transforms\n",
    "\n",
    "We are using the datasets.MNIST function from torchvision to import the dataset. \n",
    "\n",
    "Parameters to the function:\n",
    "\n",
    "The root parameter sets the directory that we import the data to (and create it if it doesn't exist.)\n",
    "\n",
    "The train parameter determines if we are importing training or testing fashion MNIST dataset. \n",
    "\n",
    "The transform parameter determines the transforms we apply during preprocessing, which were defined above.\n",
    "\n",
    "download=True gives the function permission to download the data into the directory if it doesn't exist there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "Q_fPpFanuNAa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_set\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39m\u001b[43mtransform\u001b[49m) \u001b[38;5;66;03m# downloads to train_set\u001b[39;00m\n\u001b[1;32m      2\u001b[0m valset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_set\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform) \u001b[38;5;66;03m# downloads to test_set\u001b[39;00m\n\u001b[1;32m      3\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# batch is the number of images to consider at a time\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.MNIST('train_set', download=True, train=True, transform=transform) # downloads to train_set\n",
    "valset = torchvision.datasets.MNIST('test_set', download=True, train=False, transform=transform) # downloads to test_set\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # batch is the number of images to consider at a time\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0T4Y-JVy4WUy"
   },
   "source": [
    "## Visualization !!!\n",
    "\n",
    "For 10 iterations, select a random index from zero to the length of the training dataset. Display each image in a 2 by 5 subplot as the 1st, 2nd, 3rd etc... image in the plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "collapsed": false,
    "id": "AwWg6nMm4gdg",
    "outputId": "bfd04d34-6ca8-43bc-8830-e3978ec1316e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     rand_idx \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trainset)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Pick an index for a random image\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 2 * 5 images in the grid, display 10 total\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    rand_idx = random.randint(0, len(trainset)-1) # Pick an index for a random image\n",
    "    plt.subplot(2, 5, i+1) # 2 * 5 images in the grid, display 10 total\n",
    "    plt.axis('off')\n",
    "    # [1, 28, 28] -> [28, 28]\n",
    "    plt.imshow(trainset[rand_idx][0].squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pc8-Fbf4Dkvb"
   },
   "source": [
    "<h1>Creating the Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_xDFVY4muq_1"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "datasets docs - https://pytorch.org/vision/stable/datasets.html\n",
    "\n",
    "The input size is the pixel size of the images, each of which is 28 by 28.\n",
    "\n",
    "The number of classifcation (num_classes) is 10 because there are 10 possible classifications the model can make, such as 0, 1, 2 ... 9. \n",
    "\n",
    "The hidden_sizes is the number of neurons in the hidden layer of the neural network. The input size and output sizes are always fixed (input size is related to number of pixels and output size is the number of classes) but the optimal hidden layer sizes can be determined only through experimentation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "id": "Bg20RWXHuW2W"
   },
   "outputs": [
   ],
   "source": [
    "input_size = 28 * 28 # width times height of the image (number of pixels)\n",
    "hidden_sizes = [128, 32] # this is the sizes of the hidden layers. The sizes are relativly arbitrary\n",
    "num_class = 10 # one label for each digit (0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "E9WnnZwZxGWq"
   },
   "source": [
    "GPU boost training time. Why? Because it lets us do many operations at the same time in a parallelized sort of way. \n",
    "\n",
    "CUDA is the API that we will use for GPU training. If CUDA is available we want to use it, and otherwise use the CPU. Google colab comes with a built in GPU for use so make sure to activate it by going to Runtime->Change runtime type->GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "ATekqXGWxKBs"
   },
   "outputs": [
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hRpV2GTwvBYZ"
   },
   "source": [
    "Build a neural network in pytorch with two hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QtFMastrDkvf"
   },
   "source": [
    "So torch.nn.Sequential pretty much compiles a group of layers into one network, and runs them sequentially for predictions. All of the layers that we are using for this are fully connected layers. The input size is 28 by 28 because we are flattening out the 28 by 28 image into 28*28=784 numbers. We are adding a activation function ReLU after that. ReLU(x) = max(x,0) so relu turns negative values to zero and positive values stay the same. \n",
    "\n",
    "After the first layer, we take the number of outputs of the first layer as the number of inputs into the second layer. And an arbitrary number of outputs for the second layer that we decide. \n",
    "\n",
    "Notice how we have an activation function after each layer. \n",
    "\n",
    "Then the third layer you code yourself, view the instructions below. \n",
    "\n",
    "Finally, we have an output activation function. The 10 numbers that are outputted from our previous layer can be any numbers from negative infinity to infinity. We want every class output to be from zero to one, like a probability. The softmax activation function turns the output into a probability for each class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "id": "iWS_Hzt-uhNP"
   },
   "outputs": [
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    # [(64) batch_size, (768) width x height] -> [(64) batch_size, (128) hidden_size #1]\n",
    "    torch.nn.Linear(input_size, hidden_sizes[0]),\n",
    "    torch.nn.ReLU(), # activation function\n",
    "    # [(64) batch_size, (128) hidden_size #1] -> [(64) batch_size, (32) hidden_size #2]\n",
    "    torch.nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "    torch.nn.ReLU(), # activation function\n",
    "    # [(64) batch_size, (32) hidden_size #2] -> [(64) batch_size, (10) num_class]\n",
    "\n",
    "    #EXERCISE: Define the third layer as taking in the output size of the second layer and outputting the number of classes. \n",
    "    torch.nn.Linear(hidden_sizes[1], num_class),\n",
    "    # output activation function - the hidden layer functions don't work for optimization\n",
    "    torch.nn.ReLU(),\n",
    "    # LogSoftmax because it is better at gradient optimization\n",
    "    torch.nn.LogSoftmax(dim=-1) # apply LogSoftmax to the last layer (num_class)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wqegJQ24Dkvg"
   },
   "source": [
    "We have to transfer the model to the GPU device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "zqtJo49FxLH-",
    "outputId": "3b8bc56d-ab7f-4a57-da50-4eb6d2f4651e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=32, out_features=10, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "N1UvEyWzDkvi"
   },
   "source": [
    "<h1>Training the model</h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1HqqlulNwyqb"
   },
   "source": [
    "## Training our Model\n",
    "\n",
    "The training process goes somewhat like this\n",
    "go through the dataset [epoch] times<br>\n",
    "&ensp;  go through each image in the dataset<br>\n",
    "&ensp;&ensp; transfer inputs and labels to GPU<br>\n",
    "&ensp;&ensp; get prediction for input<br>\n",
    "&ensp;&ensp; check if prediction matches label, get loss<br>\n",
    "&ensp;&ensp; see which direction you have to change the weights<br>\n",
    "&ensp;&ensp; actually change weights using optimizer and learning rate\n",
    "&ensp;&ensp; Set the directions back to zero (optim.zero_grad())<br>\n",
    "&ensp;&ensp; add loss to total loss until reset<br>\n",
    "&ensp;&ensp; after some iterations, print out loss and reset\n",
    "\n",
    "Few more things to note. NLLLoss is the default loss function for softmax (probabilities that are far away from the true probabilities are penalized). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wpYVqK89Dkvj"
   },
   "source": [
    "NLLLoss docs - https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss\n",
    "Optim docs - https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "s-B0laEkwlst",
    "outputId": "b6c2fd03-4761-42fe-ad5f-7087096b3a88"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Model training for 8 times\u001b[39;00m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mNLLLoss() \u001b[38;5;66;03m# Loss object to find back propagation\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m) \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Model training for 8 times\n",
    "criterion = torch.nn.NLLLoss() # Loss object to find back propagation\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9) # Optimizer\n",
    "epochs = 8\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        #EXERCISE: transfer images and labels to GPU. \n",
    "        inputs = inputs.to(divice)\n",
    "        labels = labels.to(divice)\n",
    "        # Formats the image to be a usable 1d array.\n",
    "        #   [(64) batch size, 1, 28, 28] -> [(64) batch size, 768]\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        # Resets the optimizer for each training step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Finds error then runs back propagation\n",
    "        output = model(images)\n",
    "        #EXERCISE: calculate the loss by passing the outputs and the labels into the lsos function. \n",
    "        loss = criterion(output,labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Keeps track of error to allow visualization of progress\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "S_58fHPtDkvk"
   },
   "source": [
    "## Evaluation Loop\n",
    "&ensp; Iterate through every batch in the dataset<br>\n",
    "&ensp;&ensp; Get the prediction of every image in the batch<br>\n",
    "&ensp;&ensp; Add the number of images to the total<br>\n",
    "&ensp;&ensp; Add the number of correctly classified images to a counter<br>\n",
    "&ensp;&ensp; Get the accuracy through correct/total.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "Ppm0FK4xyJQR",
    "outputId": "687c617f-975b-4636-f2e6-f3f321837f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 10000\n",
      "\n",
      "Model Accuracy = 0.9664\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        #EXERCISE: Transfer the inputs and labels to the GPU\n",
    "        #EXERCISE: calculate outputs by running images through the network\n",
    "        \n",
    "        images = images.view(images.shape[0], -1)\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FU9DtjN9Dkvm"
   },
   "source": [
    "<h1>Using the Model!</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ffF-vY365-Dn"
   },
   "source": [
    "Got your own image to run prediction on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "collapsed": false,
    "id": "9CrgNFgI3xYt",
    "outputId": "3b10d270-b394-4a36-f944-2b57006315cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f84769e7690>"
      ]
     },
     "execution_count": 0,
     "metadata": {
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAGAElEQVR4nO3dS4iVZRzH8XPG0ckZu5dFVkpFkZWtuhGBQUNESIsaKshdF6qFtahFIARFUSRht2UtohtWEHQDCSoisyiyyAS7EZJlmaXlLZvTWvD9a3NmnN/MfD5Lf71njujXB3o4M+1Op9MC8vSM9xsA9k2cEEqcEEqcEEqcEKq3Ggd7hvyvXBhjK4dXtPf1605OCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCCVOCNU73m9grGy+6aLG7eTF35TPrtt0XLnv3jW93Oe8UO/9G/5q3IY/X1s+y9Th5IRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQk/ae8+67nm/crh7YUj98apdffGE9/7Bne+O2/NdLu/ziE9fHm+Y2bgPLDi+f7X3n09F+O+POyQmhxAmhxAmhxAmhxAmhxAmhxAmh2p1Op3Ec7BlqHsP9fc0FjdtvC+p/k478uv5tbzmzXe4zFvxR7g+f/WrjNjhzR/nsG9tnlfuV/c2fFe3Wjs7ucl+9a6DcFx7yz4i/9mlv3FLup9/8yYhfe7ytHF6xz79QTk4IJU4IJU4IJU4IJU4IJU4IJU4INWk/zznw8upi6+61D+vu8dbjxy9s3O6/eF79td+rv+fuwwtPG8E7OjC9O4bLfeCLjeV+9PuvlPs5M5q/32//D/X3Ap6MnJwQSpwQSpwQSpwQSpwQSpwQSpwQatLecybb8/MvjdvAK81bq9Vq/buf1x54efMI3tHo+OXG5p+J2mq1WmfNqP+6PfL7GY3bvGe+K5/dU64Tk5MTQokTQokTQokTQokTQokTQrlK4YD1zj2p3J+454lyn96eVu4rll/WuB29cVX57GTk5IRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ7jk5YOvunFPu5/XVPxrxq931jzc8au32//2eJjMnJ4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4Ryz8ledl15XuP22TWP7ufpvnK9dcmScp/54cf7ef2pxckJocQJocQJocQJocQJocQJocQJodxzspcfr2j+93pWu77HvP77wXLvf3tNuXfKdepxckIocUIocUIocUIocUIocUIocUIo95xTTM+hh5b74ks+aNy2Du8sn930wCnl3rfrk3Jnb05OCCVOCCVOCCVOCCVOCCVOCOUqZYpZf+9Z5f76MU81bletv7p8tu9NVyWjyckJocQJocQJocQJocQJocQJocQJodxzTjJ/3nBhuX9x7WPl/u2efxq3vx46sXy2r7Wx3Pl/nJwQSpwQSpwQSpwQSpwQSpwQSpwQyj3nBNM754Ryv2PpS+Xe167/yK9bs7hxO/Ytn9c8mJycEEqcEEqcEEqcEEqcEEqcEEqcEMo9Z5h2b/1Hcu7rG8p9aNbmcn9u2+xyP25p87/Xw+WTjDYnJ4QSJ4QSJ4QSJ4QSJ4QSJ4RylZLm3DPK+b7Zz3b18k8+MFTuR6xZ1dXrM3qcnBBKnBBKnBBKnBBKnBBKnBBKnBDKPec4mDb/9Mbt5hdf6+q15z99e7nPe/ajrl6fg8fJCaHECaHECaHECaHECaHECaHECaHcc46Ddbcd2bgt6t/a1Wuf+O7u+j/odLp6fQ4eJyeEEieEEieEEieEEieEEieEEieEcs85BnYuOr/c31m0rFj7R/fNMGE5OSGUOCGUOCGUOCGUOCGUOCGUOCGUe84x8NPF08r95N6R32U+t212uU/fWn+e06c5Jw4nJ4QSJ4QSJ4QSJ4QSJ4QSJ4RylRLmwc3zy33V5fPKvbPxy1F8N4wnJyeEEieEEieEEieEEieEEieEEieEaneKHwk32DPkE0YwxlYOr2jv69ednBBKnBBKnBBKnBBKnBBKnBBKnBCqvOcExo+TE0KJE0KJE0KJE0KJE0KJE0L9B9RLt7c+v3l9AAAAAElFTkSuQmCC",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get and show a sample image\n",
    "image = valset[0][0] # shape: [(1) batch_size, 28, 28]\n",
    "plt.subplot()\n",
    "plt.axis('off')\n",
    "# [1, 28, 28] -> [28, 28] - makes the image readable\n",
    "plt.imshow(image.squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "q2ZVmfom6XsO"
   },
   "source": [
    "Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": false,
    "id": "2hxLIQGI6HHa",
    "outputId": "53825ba4-27c7-424b-d6c9-63a7eccbdcf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 7\n"
     ]
    }
   ],
   "source": [
    "# [(1) batch_size, 28, 28]-> [1(batch size), 784]\n",
    "image = image.view(1, 784)\n",
    "with torch.no_grad():\n",
    "    # cast input to device\n",
    "    image = image.to(device)\n",
    "\n",
    "    log_preds = model(image) # runs the model\n",
    "\n",
    "# post processes the image into probabilities of it being each digit\n",
    "#   Math: probabilites were natural logged, so torch.exp() performs e^(log_preds)\n",
    "preds = torch.exp(log_preds)\n",
    "probab = list(preds.cpu().numpy()[0])\n",
    "\n",
    "# the index, this time, is the same as the output, so we can just grab and print it\n",
    "pred_label = probab.index(max(probab)) # get index of highest num (highest probability)\n",
    "print(f\"Prediction: {pred_label}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
   ],
   "name": "Digits_Classifier.ipynb",
   "provenance": [
   ]
  },
  "kernelspec": {
   "display_name": "Python (cv_env)",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "cv_env",
   "resource_dir": "/projects/7054eb54-2282-47cc-9fd4-f73d8c0d026c/.local/share/jupyter/kernels/cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}